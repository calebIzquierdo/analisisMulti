 library(nnet)
> 
> # Normalizar los predictores individualmente
> diabetes$Pregnancies = scale(diabetes$Pregnancies)
> diabetes$Glucose = scale(diabetes$Glucose)
> diabetes$BloodPressure = scale(diabetes$BloodPressure)
> diabetes$SkinThickness = scale(diabetes$SkinThickness)
> diabetes$Insulin = scale(diabetes$Insulin)
> diabetes$BMI = scale(diabetes$BMI)
> diabetes$DiabetesPedigreeFunction = scale(diabetes$DiabetesPedigreeFunction)
> diabetes$Age = scale(diabetes$Age)
> 
> # Crear el modelo de red neuronal
> modelo = nnet(Outcome ~ Pregnancies + Glucose + BloodPressure + 
+                 SkinThickness + Insulin + BMI + 
+                 DiabetesPedigreeFunction + Age,
+               data = diabetes,
+               size = 3, maxit = 500, decay = 0.01, linout = FALSE)
# weights:  31
initial  value 178.544032 
iter  10 value 117.321538
iter  20 value 113.028899
iter  30 value 111.128324
iter  40 value 108.253587
iter  50 value 106.872124
iter  60 value 106.349228
iter  70 value 106.260399
iter  80 value 106.075158
iter  90 value 106.041702
iter 100 value 106.038568
final  value 106.038567 
converged
> 
> print(modelo)
a 8-3-1 network with 31 weights
inputs: Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age 
output(s): Outcome 
options were - decay=0.01
> 
> # Hacer predicciones
> predicciones = predict(modelo, diabetes, type = "raw")
> print(predicciones)
           [,1]
1   0.526775115
2   0.012022600
3   0.961944321
4   0.016452193
5   0.796385779
6   0.042216019
7   0.019282833
8   0.405357241
9   0.786526091
10  0.166638593
11  0.235242385
12  0.867628338
13  0.717137178
14  0.787528450
15  0.736468188
16  0.526641860
17  0.632160724
18  0.283895419
19  0.463636869
20  0.460849366
21  0.182415811
22  0.211968106
23  0.938276366
24  0.296590840
25  0.918839704
26  0.476383157
27  0.887206609
28  0.028506027
29  0.404057663
30  0.093078545
31  0.230500609
32  0.663895139
33  0.025828501
34  0.057207317
35  0.549388589
36  0.092076538
37  0.631389996
38  0.531445303
39  0.084753547
40  0.642959327
41  0.795995376
42  0.839248798
43  0.075627431
44  0.824660320
45  0.712851690
46  0.920776462
47  0.711387080
48  0.044259238
49  0.342809835
50  0.010613009
51  0.021717735
52  0.024465093
53  0.025114507
54  0.775953825
55  0.783567800
56  0.008023012
57  0.792811557
58  0.427697800
59  0.783164622
60  0.078272251
61  0.006372862
62  0.665312758
63  0.012234961
64  0.348383913
65  0.261839414
66  0.034718220
67  0.426634061
68  0.580268308
69  0.009582510
70  0.121100301
71  0.079604015
72  0.272860136
73  0.662385227
74  0.178884721
75  0.025639394
76  0.010016799
77  0.061624196
78  0.305009609
79  0.753845738
80  0.019384295
81  0.020608568
82  0.006026381
83  0.143775925
84  0.010989283
85  0.749925635
86  0.082295336
87  0.195918971
88  0.088601600
89  0.549519529
90  0.015050003
91  0.007677577
92  0.183267110
93  0.257463340
94  0.239258981
95  0.109828694
96  0.725216176
97  0.021751627
98  0.007227330
99  0.249312646
100 0.487987256
101 0.792596990
102 0.232361162
103 0.041231241
104 0.009901401
105 0.314972297
106 0.133606651
107 0.012302534
108 0.672038771
109 0.098058555
110 0.033051595
111 0.561583697
112 0.784285506
113 0.014661209
114 0.215520219
115 0.797118696
116 0.491415852
117 0.287991599
118 0.303735066
119 0.210091701
120 0.086582239
121 0.908892931
122 0.346819193
123 0.088504429
124 0.215051155
125 0.055113917
126 0.831108592
127 0.388685033
128 0.078936715
129 0.620028581
130 0.228435795
131 0.770026193
132 0.685562708
133 0.775960283
134 0.197472774
135 0.014509138
136 0.442695094
137 0.039205500
138 0.025640090
139 0.456316335
140 0.135870771
141 0.231348247
142 0.184439354
143 0.076582423
144 0.284810338
145 0.184513588
146 0.006374448
147 0.135632081
148 0.580429681
149 0.376640075
150 0.016859350
151 0.273631105
152 0.103526313
153 0.977405129
154 0.322296311
155 0.984209659
156 0.970950470
157 0.039051429
158 0.066671078
159 0.025797176
160 0.948445013
161 0.542410250
162 0.219863951
163 0.163514778
164 0.068651047
165 0.617059025
166 0.158833066
167 0.417346771
168 0.231953521
169 0.144888349
170 0.046932099
171 0.047944453
172 0.648828624
173 0.120865319
174 0.281109198
175 0.009990556
176 0.975211889
177 0.017894275
178 0.741202286
179 0.738140932
180 0.880130936
181 0.025078852
182 0.156419798
183 0.021440767
184 0.090796807
185 0.529572521
186 0.933745171
187 0.784681049
188 0.648276527
189 0.381347649
190 0.388958229
191 0.033080752
192 0.616537850
193 0.896938000
194 0.988101405
195 0.026336994
196 0.653293268
197 0.018065536
198 0.048990936
199 0.508356588
200 0.462519250
201 0.082707525
202 0.342887031
203 0.288225693
204 0.009606178
205 0.354127077
206 0.085723382
207 0.785454141
208 0.718740317
209 0.041733837
210 0.901093801
211 0.011655353
212 0.647772376
213 0.592289687
214 0.595723827
215 0.472687480
216 0.865492720
217 0.537956222
218 0.240411452
219 0.203656438
220 0.385003563
221 0.767332565
222 0.610825726
223 0.381490427
224 0.510863605
225 0.022679418
226 0.026083305
227 0.036587674
228 0.964567087
229 0.787546170
230 0.121320757
231 0.727869836
232 0.775635446
233 0.013127669
234 0.299602698
235 0.058037278
236 0.953081158
237 0.782425465
238 0.837464277
239 0.809782584
240 0.016921278
241 0.016044485
242 0.346317714
243 0.145326615
244 0.596186129
245 0.635181440
246 0.907448545
247 0.416946549
248 0.494213229
249 0.477767772
250 0.027561504
251 0.385123274
252 0.061908244
253 0.012827330
254 0.013826357
255 0.457205820
256 0.116282238
257 0.102275285
258 0.021853586
259 0.784722298
260 0.977485840
261 0.783304050
262 0.797467124
263 0.277855057
264 0.293911633
265 0.269943326
266 0.322231081
267 0.778023821
268 0.857201675
269 0.014688287
270 0.625027218
271 0.576355723
272 0.018591482
273 0.255550091
274 0.034510040
275 0.428105117
276 0.322436395
277 0.194415072
278 0.040476260
279 0.245368638
280 0.082384532
281 0.710236134
282 0.646345279
283 0.204280239
284 0.661676974
285 0.227049958
286 0.503911941
287 0.660795291
288 0.329729459
289 0.013860578
290 0.133788106
291 0.042081910
292 0.155865943
293 0.741530945
294 0.597303309
295 0.281863437
296 0.877604754
297 0.641487982
298 0.118807701
299 0.390536804
300 0.232000387
301 0.773316381
302 0.496871828
303 0.042018720
304 0.709518590
305 0.512594790
306 0.171918295
307 0.711481267
308 0.105148901
309 0.495144646
310 0.495699144
311 0.015032084
312 0.098462008
313 0.536673997
314 0.108620298
315 0.852945622
316 0.068315497
317 0.009902110
318 0.770031742
319 0.112081010
320 0.633907166
321 0.274517072
322 0.074586320
323 0.352993566
324 0.718278705
325 0.208997995
326 0.425613656
327 0.620161461
328 0.976066600
329 0.403417397
330 0.032593814
331 0.620585058
332 0.020458685
333 0.774613311
334 0.160295317
335 0.012776952
336 0.773087018
337 0.480260992
338 0.304710740
339 0.820967724
340 0.941858491
341 0.083795177
342 0.138442947
343 0.053802681
344 0.216251469
345 0.254657156
346 0.634289230
347 0.436341608
348 0.084614448
349 0.012023366
350 0.122213016
351 0.402139726
352 0.137525789
353 0.046973622
354 0.019215230
355 0.626489680
356 0.753945832
357 0.702090949
358 0.950367248
359 0.196640400
360 0.787372727
361 0.781588854
362 0.404860454
363 0.206240944
364 0.523867592
365 0.392940544
366 0.216114688
367 0.333578537
368 0.010397849
369 0.056602319
370 0.632958511
371 0.797205633
372 0.038125926
373 0.044587510
374 0.046894237
375 0.608231190
376 0.827031404
377 0.022232327
378 0.076898857
379 0.958246847
380 0.300500769
381 0.066425992
382 0.010137828
383 0.079270987
384 0.036907501
385 0.062344670
386 0.034502432
387 0.271984568
388 0.396181639
389 0.717348755
390 0.112407250
391 0.444393712
392 0.898404204
393 0.097491328
394 0.132803850
395 0.794190728
396 0.332406885
397 0.257306305
398 0.142963092
399 0.011661076
400 0.873537605
401 0.028782442
402 0.254129616
403 0.442657047
404 0.149985288
405 0.735020947
406 0.541414861
407 0.305204202
408 0.014665028
409 0.950947118
410 0.783449857
411 0.402196840
412 0.091005354
413 0.179562796
414 0.092227243
415 0.347826920
416 0.164534467
417 0.060821931
418 0.781403407
419 0.008228545
420 0.099438134
421 0.166084528
422 0.039444496
423 0.111835498
424 0.134910248
425 0.941683712
426 0.779950441
427 0.064142082
428 0.781196923
429 0.274826818
430 0.386067380
431 0.021630723
432 0.046975016
433 0.026872896
434 0.179868869
435 0.156471490
436 0.744907938
437 0.562250605
438 0.448275216
439 0.008199557
440 0.590799929
441 0.775306824
442 0.077619380
443 0.257992920
444 0.514925673
445 0.077249130
446 0.990335081
447 0.030048804
448 0.028596513
449 0.046683848
450 0.104079397
451 0.010526790
452 0.122480810
453 0.067369519
454 0.190362887
455 0.148435386
456 0.875190779
457 0.300579984
458 0.223805600
459 0.918246531
460 0.193904025
461 0.252673496
462 0.007191156
463 0.175830257
464 0.011388154
465 0.496451652
466 0.075358310
467 0.009377704
468 0.064510265
469 0.776767483
470 0.827670494
471 0.745574058
472 0.170047367
473 0.108696207
474 0.413528470
475 0.152715267
476 0.238871295
477 0.081672011
478 0.163217590
479 0.103779157
480 0.222097050
481 0.777908062
482 0.177352513
483 0.023347813
484 0.034807764
485 0.774738054
486 0.483624904
487 0.194211318
488 0.786394608
489 0.023351557
490 0.673326828
491 0.147997896
492 0.112269815
493 0.025509239
494 0.676527058
495 0.006025096
496 0.402490401
497 0.047169403
498 0.025960353
499 0.717115268
500 0.768467125
501 0.050432245
502 0.080539778
503 0.126017853
504 0.217798563
505 0.136123279
506 0.178038523
507 0.781282711
508 0.124063538
509 0.165486325
510 0.210924230
511 0.154336882
512 0.149408855
513 0.179068510
514 0.042381288
515 0.018000913
516 0.663948009
517 0.685438678
518 0.501129946
519 0.161153194
520 0.587265834
521 0.007611907
522 0.099451343
523 0.009791886
524 0.932345380
525 0.132687647
526 0.034982280
527 0.011110180
528 0.040518764
529 0.094402732
530 0.246287751
531 0.121034639
532 0.240990712
533 0.116822438
534 0.317413122
535 0.086940853
536 0.881765078
537 0.264453353
538 0.183832693
539 0.164035290
540 0.449764278
541 0.552591651
542 0.135888403
543 0.640728350
544 0.338664831
545 0.015837281
546 0.898936329
547 0.786741883
548 0.128796649
549 0.646275406
550 0.776141675
551 0.025376729
552 0.078195638
553 0.204204215
554 0.017771075
555 0.033522836
556 0.078532505
557 0.035242840
558 0.209664635
559 0.290552793
560 0.194572410
561 0.316332996
562 0.787403394
563 0.043271866
564 0.080648981
565 0.028974213
566 0.045210123
567 0.201289855
568 0.154265780
569 0.731069373
570 0.631743619
571 0.017237930
572 0.055446713
573 0.145249888
574 0.076510477
575 0.149612570
576 0.298722462
577 0.136747143
578 0.669852546
579 0.428492594
580 0.490158327
581 0.422562889
582 0.143864066
583 0.246561853
584 0.274665009
585 0.773209017
586 0.011130226
587 0.884029795
588 0.098753380
589 0.780895144
590 0.021929832
591 0.722373049
592 0.116354753
593 0.601151219
594 0.142447026
595 0.379134042
596 0.750081299
597 0.221659790
598 0.020546589
599 0.767476071
600 0.097208341
601 0.023622546
602 0.088687896
603 0.132057170
604 0.705928304
605 0.739535856
606 0.235645967
607 0.778171253
608 0.008971447
609 0.716905645
610 0.042331977
611 0.065630984
612 0.783208537
613 0.815800150
614 0.487568948
615 0.960002121
616 0.019614201
617 0.241264549
618 0.007479650
619 0.724529085
620 0.488661817
621 0.077501889
622 0.060439615
623 0.907314961
624 0.117898484
625 0.078127110
626 0.177063417
627 0.044079659
628 0.093899450
629 0.464781585
630 0.104668595
631 0.342256928
632 0.031857538
633 0.034639355
634 0.072841801
635 0.187532752
636 0.288664266
637 0.204678344
638 0.094132872
639 0.593755568
640 0.009933734
641 0.053430911
642 0.519300368
643 0.487464946
644 0.177578469
645 0.050587164
646 0.737462457
647 0.763524960
648 0.784969576
649 0.518095387
650 0.015202190
651 0.012332314
652 0.222895995
653 0.352157694
654 0.086902534
655 0.052366471
656 0.644710922
657 0.011057293
658 0.753437837
659 0.734183524
660 0.343494400
661 0.555491565
662 0.975436604
663 0.796878525
664 0.964843879
665 0.354455173
666 0.053201896
667 0.257494294
668 0.180051611
669 0.283855912
670 0.743046455
671 0.764079855
672 0.023339450
673 0.139928102
674 0.728524431
675 0.204729583
676 0.928091012
677 0.452345707
678 0.028253802
679 0.227727409
680 0.060678364
681 0.011357559
682 0.848891595
683 0.118780105
684 0.354710129
685 0.186704135
686 0.128773833
687 0.067073004
688 0.073281520
689 0.184287701
690 0.760592529
691 0.331062900
692 0.740804117
693 0.469731993
694 0.772657422
695 0.009761501
696 0.633167197
697 0.750231840
698 0.079970484
699 0.185561605
700 0.724085705
701 0.115155087
702 0.271936517
703 0.737242903
704 0.538441788
705 0.039679049
706 0.213925032
707 0.039502898
708 0.121482446
709 0.893154866
710 0.200912080
711 0.232961921
712 0.340994133
713 0.738738427
714 0.171985070
715 0.016485183
716 0.815783214
717 0.787773614
718 0.193781701
719 0.083474683
720 0.216069944
721 0.011679068
722 0.097768508
723 0.675832602
724 0.444132568
725 0.360382730
726 0.272660397
727 0.098361275
728 0.147449206
729 0.184839785
730 0.030982134
731 0.343257709
732 0.556513804
733 0.869915339
734 0.055595059
735 0.218613052
736 0.147398829
737 0.073953827
738 0.121138481
739 0.179263773
740 0.415451814
741 0.955456555
742 0.050183803
743 0.040670307
744 0.911744230
745 0.792347663
746 0.217496271
747 0.820859589
748 0.259832720
749 0.786096481
750 0.485547202
751 0.697896704
752 0.195900950
753 0.021538523
754 0.772249250
755 0.798872964
756 0.739812586
757 0.615795143
758 0.409367884
759 0.056663942
760 0.726957068
761 0.066209448
762 0.987928307
763 0.147783550
764 0.206333356
765 0.166392408
766 0.047856268
767 0.384603391
768 0.016498777
> 
> # Convertir probabilidades a clases (0 o 1)
> predicciones_clase = ifelse(predicciones > 0.5, 1, 0)
> 
> # Comparar con los valores reales
> tabla = table(Predicho = predicciones_clase, Real = diabetes$Outcome)
> print(tabla)
        Real
Predicho   0   1
       0 437  88
       1  63 180
> 
> # Medir precisión
> Precision = sum(diag(tabla)) / sum(tabla)
> cat("Precisión del modelo:", round(Precision * 100, 2), "%\n")
Precisión del modelo: 80.34 %
